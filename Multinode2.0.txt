Create a t2.micro instance 
wget https://s3.amazonaws.com/shaan/My+Self/alam.pem
sudo apt-get update && sudo apt-get dist-upgrade -y
cp alam.pem ~/.ssh
sudo -i
passwd
exit
-----------
sudo apt-get install -y python-software-properties debconf-utils
sudo add-apt-repository -y ppa:webupd8team/java
sudo apt-get update
echo "oracle-java8-installer shared/accepted-oracle-license-v1-1 select true" | sudo debconf-set-selections
sudo apt-get install -y oracle-java8-installer
--------------------
wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz 
sudo tar xzvf hadoop-2.7.2.tar.gz
sudo mv hadoop-2.7.2 /usr/local/hadoop
------------------------------------
cat >>$HOME/.bashrc <<EOL
# -- HADOOP ENVIRONMENT VARIABLES START -- #
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_HOME=/usr/local/hadoop
export PATH=\$PATH:\$HADOOP_HOME/bin
export PATH=\$PATH:\$HADOOP_HOME/sbin
export PATH=\$PATH:/usr/local/hadoop/bin/
export HADOOP_MAPRED_HOME=\$HADOOP_HOME
export HADOOP_COMMON_HOME=\$HADOOP_HOME
export HADOOP_HDFS_HOME=\$HADOOP_HOME
export YARN_HOME=\$HADOOP_HOME
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export PDSH_RCMD_TYPE=ssh
# -- HADOOP ENVIRONMENT VARIABLES END -- #
EOL

 exec bash
 sudo chown -R ubuntu:ubuntu /usr/local/hadoop
-----------------------------------------------
sudo su -c 'echo export JAVA_HOME=/usr/lib/jvm/java-8-oracle >> /usr/local/hadoop/etc/hadoop/hadoop-env.sh'

sudo su -c 'echo export HADOOP_LOG_DIR=/var/log/hadoop/ >> /usr/local/hadoop/etc/hadoop/hadoop-env.sh'

sudo mkdir /var/log/hadoop/

sudo chown ubuntu:ubuntu -R /var/log/hadoop
---------------------------------------------
cat /proc/sys/net/ipv6/conf/all/disable_ipv6
sudo ufw disable
sudo sysctl -w vm.swappiness=0
sudo tune2fs -m 0 /dev/xvda1
----------------------------------
cat /sys/kernel/mm/transparent_hugepage/defrag
sudo sed -i '/exit 0/d' /etc/rc.local

sudo su -c 'cat >>/etc/rc.local <<EOL
if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
  echo never > /s	ys/kernel/mm/transparent_hugepage/enabled
fi
if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
   echo never > /sys/kernel/mm/transparent_hugepage/defrag
fi
exit 0
EOL'

sudo -i
source /etc/rc.local
------------------------------------------------------
sudo timedatectl set-timezone Asia/Kolkata
sudo apt-get install ntp
sudo ntpq -p
-----------------------------------------
sudo su -c touch /home/ubuntu/.ssh/config; echo "Host *\n StrictHostKeyChecking no\n  UserKnownHostsFile=/dev/null" > /home/ubuntu/.ssh/config
echo -e  'y\n'| ssh-keygen -t rsa -P "" -f $HOME/.ssh/id_rsa
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
---------------------------------------
sudo service ssh restart
#######################################################
	Create Snapshot
#######################################################

sudo nano /etc/hosts
(Add the FQDN OF dn & rm after create image and instance)
172.31.27.147	ip-172-31-27-147.us-west-2.compute.internal	nn
172.31.22.85	ip-172-31-22-85.us-west-2.compute.internall	dn
172.31.22.86	ip-172-31-22-86.us-west-2.compute.internall	rm
----------------------------------
sudo chown ubuntu:ubuntu -R /var/log/hadoop
sudo apt-get install pdsh -y
sudo nano /etc/genders
	(nn,dn,rm)
-----------------------------
nano /usr/local/hadoop/etc/hadoop/slaves
sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/core-site.xml

sudo su -c 'cat >> /usr/local/hadoop/etc/hadoop/core-site.xml <<EOL
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://172.31.27.147:9000</value>
  </property>
</configuration>

EOL'
-------------------------
mkdir -p /usr/local/hadoop/data/hdfs/namenode
sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/hdfs-site.xml

sudo su -c 'cat >>/usr/local/hadoop/etc/hadoop/hdfs-site.xml <<EOL
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/namenode</value>
  </property>
</configuration>
EOL'
------------------------------------------------------
================================================
Note:-On Data Node--

mkdir -p /usr/local/hadoop/data/hdfs/datanode

sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/hdfs-site.xml

sudo su -c 'cat >>/usr/local/hadoop/etc/hadoop/hdfs-site.xml <<EOL
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/datanode</value>
  </property>
</configuration>
EOL'

===============================================
sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/yarn-site.xml

sudo su -c 'cat >>/usr/local/hadoop/etc/hadoop/yarn-site.xml <<EOL

<configuration>
<!-- Site specific YARN configuration properties -->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>172.31.22.86</value>
  </property>
</configuration>
EOL'

Note:->u have to change RM ip in yarn-site.xml
----------------------------------------------------------------
cp  /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml
sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/mapred-site.xml

sudo su -c 'cat >>/usr/local/hadoop/etc/hadoop/mapred-site.xml <<EOL
<configuration>
  <property>
    <name>mapreduce.jobtracker.address</name>
    <value>172.31.22.86:54311</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
EOL'

Note:->u have to change RM ip in mapred-site.xml
-----------------------------------------------------------------------
sudo chown -R ubuntu:ubuntu $HADOOP_HOME


#SCP all the files
cd /usr/local/hadoop/etc/hadoop && scp core-site.xml mapred-site.xml yarn-site.xml hadoop-env.sh slaves dn:/usr/local/hadoop/etc/hadoop
cd /usr/local/hadoop/etc/hadoop && scp core-site.xml mapred-site.xml yarn-site.xml hdfs-site.xml hadoop-env.sh slaves rm:/usr/local/hadoop/etc/hadoop
#Format Namenode

hdfs namenode -format

start-dfs.sh
start-yarn.sh

-----------------------------
pdsh -a jps
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/ubuntu
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar teragen 500000 random-data
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar terasort random-data sorted-data
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-*-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 5MB
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-*-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 5MB




#Troubleshooting:-


 cp VERSION /usr/local/hadoop/data/hdfs/datanode/





























